#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PostgreSQL to Oracle 프로시저 완전 자동 변환 도구
입력: postgres.sql + ast.json
출력: oracle.sql (완전한 변환, 축약 없음)
"""

import json
import re
import requests
import time
from pathlib import Path

# ============================================================================
# 설정 (여기를 수정하세요)
# ============================================================================

# 입력 파일 경로
SQL_FILE_PATH = "./postgres_procedure.sql"
AST_JSON_PATH = "./procedure_ast.json"

# 출력 파일 경로
OUTPUT_SQL_FILE = "./oracle_procedure.sql"

# LLM API 설정
LLM_API_URL = "https://your-company-api.com/v1/chat/completions"
LLM_API_KEY = "your-api-key-here"
LLM_MODEL = "gpt-4.1"

# LLM 호출 설정
REQUEST_TIMEOUT = 180
RETRY_COUNT = 3
DELAY_BETWEEN_CALLS = 2

# 청크 분할 기준
WARN_CHUNK_LINES = 500

# 로그 디렉토리
LOG_DIR = "./conversion_logs"

# ============================================================================
# LLM 호출 함수
# ============================================================================

def call_llm(user_message, system_message="당신은 PostgreSQL을 Oracle로 변환하는 전문가입니다.", retry=0):
    """LLM API 호출 (재시도 로직 포함)"""
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": LLM_API_KEY
    }
    
    payload = {
        "messages": [
            {
                "role": "system",
                "content": system_message
            },
            {
                "role": "user",
                "content": user_message
            }
        ],
        "model": LLM_MODEL
    }
    
    try:
        response = requests.post(
            LLM_API_URL, 
            headers=headers, 
            json=payload, 
            timeout=REQUEST_TIMEOUT
        )
        response.raise_for_status()
        
        return response.text
        
    except requests.exceptions.RequestException as e:
        print(f"  [WARNING] API 호출 실패: {e}")
        
        if retry < RETRY_COUNT:
            print(f"  [RETRY] 재시도 중... ({retry + 1}/{RETRY_COUNT})")
            time.sleep(DELAY_BETWEEN_CALLS * 2)
            return call_llm(user_message, system_message, retry + 1)
        else:
            print(f"  [ERROR] 최대 재시도 횟수 초과")
            return None

# ============================================================================
# 파일 로드
# ============================================================================

def load_files():
    """SQL 파일과 AST 파일 로드"""
    
    print("[INFO] 파일 로딩 중...")
    
    with open(SQL_FILE_PATH, 'r', encoding='utf-8') as f:
        sql_content = f.read()
        sql_lines = sql_content.split('\n')
    
    with open(AST_JSON_PATH, 'r', encoding='utf-8') as f:
        ast_data = json.load(f)
    
    print(f"[INFO] SQL 파일: {len(sql_lines)} 라인")
    print(f"[INFO] AST 구조 로드 완료")
    
    return sql_lines, ast_data

# ============================================================================
# AST 청크 추출 (리프 노드까지 재귀)
# ============================================================================

def extract_chunks_from_ast(node, chunks, chunk_id_counter, parent_context=None, depth=0):
    """
    AST를 재귀 순회하며 청크 추출
    - children이 없는 노드 = 리프 노드 = 완전한 청크
    - children이 있는 노드 = 계속 들어가기
    """
    
    if not isinstance(node, dict):
        return chunk_id_counter
    
    node_type = node.get('type', 'UNKNOWN')
    start_line = node.get('startLine', 0)
    end_line = node.get('endLine', 0)
    line_count = end_line - start_line + 1 if end_line > 0 else 0
    children = node.get('children', [])
    
    # 현재 노드의 컨텍스트 정보 생성
    current_context = {
        'type': node_type,
        'start_line': start_line,
        'end_line': end_line,
        'parent': parent_context
    }
    
    # children이 없으면 = 리프 노드 = 청크로 추가
    if not children:
        if line_count > 0:
            chunk_id_counter[0] += 1
            
            # 부모 컨텍스트 경로 생성
            context_path = []
            ctx = parent_context
            while ctx:
                context_path.insert(0, f"{ctx['type']}({ctx['start_line']}-{ctx['end_line']})")
                ctx = ctx.get('parent')
            
            context_str = ' > '.join(context_path) if context_path else 'ROOT'
            
            chunks.append({
                'chunk_id': chunk_id_counter[0],
                'name': f'{node_type}_{chunk_id_counter[0]}',
                'ast_type': node_type,
                'start_line': start_line,
                'end_line': end_line,
                'size_lines': line_count,
                'priority': 'HIGH' if node_type in ['CTE', 'SELECT', 'INSERT', 'UPDATE', 'DELETE'] else 'MEDIUM',
                'description': f'{node_type} 블록',
                'conversion_notes': f'{node_type} 처리',
                'parent_context': context_str
            })
            
            if line_count > WARN_CHUNK_LINES:
                print(f"[WARNING] 청크 {chunk_id_counter[0]} '{node_type}' ({line_count} 라인) - 큰 리프 노드")
                print(f"          경로: {context_str}")
        
        return chunk_id_counter
    
    # children이 있으면 = 컨테이너 노드 = 재귀적으로 children 처리
    else:
        if depth == 0:
            print(f"[INFO] {node_type} 노드 ({line_count} 라인) - {len(children)}개 children 순회 시작")
        
        for child in children:
            chunk_id_counter = extract_chunks_from_ast(
                child, 
                chunks, 
                chunk_id_counter, 
                current_context, 
                depth + 1
            )
        
        return chunk_id_counter

def analyze_and_chunk_direct(sql_lines, ast_data):
    """Python에서 직접 AST를 분석하여 청크 생성"""
    
    print("\n[STEP 1/3] AST 구조 분석 및 청크 분할 중...")
    print("[INFO] children이 있는 노드는 리프까지 재귀 탐색")
    
    chunks = []
    chunk_id_counter = [0]
    
    extract_chunks_from_ast(ast_data, chunks, chunk_id_counter)
    
    # 청크 정렬 (라인 순서대로)
    chunks.sort(key=lambda x: x['start_line'])
    
    # chunk_id 재할당
    for i, chunk in enumerate(chunks, 1):
        chunk['chunk_id'] = i
    
    print(f"\n[INFO] 분석 완료: {len(chunks)}개 청크로 분할")
    
    # 통계 출력
    large_chunks = [c for c in chunks if c['size_lines'] > WARN_CHUNK_LINES]
    if large_chunks:
        print(f"[WARNING] {len(large_chunks)}개의 큰 청크 (500 라인 초과) 발견")
        for chunk in large_chunks:
            print(f"  - 청크 {chunk['chunk_id']}: {chunk['ast_type']} ({chunk['size_lines']} 라인)")
    
    # 분석 결과 저장
    analysis = {
        'total_lines': len(sql_lines),
        'structure_summary': f'{len(chunks)}개 청크로 분할됨 (리프 노드 기준)',
        'chunks': chunks
    }
    
    save_log("analysis_result.json", analysis)
    
    return analysis

# ============================================================================
# 청크별 변환
# ============================================================================

def convert_chunk(chunk, sql_lines, chunk_number, total_chunks):
    """개별 청크를 Oracle로 변환"""
    
    print(f"\n  [{chunk_number}/{total_chunks}] {chunk['name']} 변환 중...")
    print(f"     라인: {chunk['start_line']} ~ {chunk['end_line']} ({chunk['size_lines']} 라인)")
    
    # SQL 추출
    start = chunk['start_line']
    end = chunk['end_line']
    sql_content = '\n'.join(sql_lines[start-1:end])
    
    # 부모 컨텍스트 정보
    context_info = f"부모 경로: {chunk['parent_context']}" if chunk.get('parent_context') else "최상위 노드"
    
    # 변환 프롬프트 생성
    prompt = f"""
PostgreSQL 프로시저의 일부를 Oracle 19c로 변환해주세요.

**청크 정보**:
- 청크 ID: {chunk['chunk_id']}/{total_chunks}
- AST 타입: {chunk['ast_type']}
- 라인 범위: {chunk['start_line']} ~ {chunk['end_line']} ({chunk['size_lines']} 라인)
- {context_info}

**주요 변환 규칙**:

1. 함수 변환:
   - COALESCE() -> NVL()
   - date_part(field, date) -> EXTRACT(field FROM date)
   - substring(str, start, len) -> SUBSTR(str, start, len)
   - now() -> SYSDATE
   - current_timestamp -> SYSTIMESTAMP
   - string_agg() -> LISTAGG()
   - array_agg() -> LISTAGG()
   - date_trunc() -> TRUNC()
   - to_char() -> TO_CHAR() (동일)
   - to_date() -> TO_DATE() (동일)

2. 타입 변환:
   - VARCHAR -> VARCHAR2
   - INTEGER -> NUMBER
   - NUMERIC(n,m) -> NUMBER(n,m)
   - BOOLEAN -> NUMBER(1)
   - TIMESTAMP -> TIMESTAMP
   - refcursor -> SYS_REFCURSOR
   - SERIAL -> NUMBER + SEQUENCE

3. 구조 변환:
   - CREATE OR REPLACE FUNCTION -> CREATE OR REPLACE PROCEDURE (RETURNS void인 경우)
   - RETURNS void 제거
   - LANGUAGE plpgsql 제거
   - $$ -> AS 또는 IS
   - END $$ -> END; /
   - PERFORM -> SELECT INTO v_dummy FROM dual
   - RAISE NOTICE -> DBMS_OUTPUT.PUT_LINE
   - RAISE EXCEPTION -> RAISE_APPLICATION_ERROR
   - CREATE TEMP TABLE -> TRUNCATE TABLE (글로벌 임시 테이블 사전 생성 필요)
   - DROP TABLE IF EXISTS -> EXECUTE IMMEDIATE 'DROP TABLE...' with EXCEPTION WHEN OTHERS THEN NULL

4. 제어 구조:
   - FOR ... IN ... LOOP -> FOR ... IN ... LOOP (동일)
   - WHILE ... LOOP -> WHILE ... LOOP (동일)
   - IF ... THEN ... ELSIF ... ELSE ... END IF -> 동일
   - EXIT WHEN -> EXIT WHEN (동일)
   - CONTINUE -> CONTINUE (동일)

5. 기타:
   - || (문자열 연결) -> || (동일)
   - LIMIT n -> FETCH FIRST n ROWS ONLY
   - OFFSET n -> ROW_NUMBER() OVER() 사용
   - RETURNING -> RETURNING INTO (Oracle)
   - := (할당) -> := (동일)

**변환할 PostgreSQL 코드**:
```sql
{sql_content}
```

**매우 중요**:
1. 변환된 Oracle 코드만 출력하세요
2. 코드 블록 마커(```)나 설명 텍스트 없이 순수 SQL만
3. 원본의 모든 로직을 빠짐없이 변환
4. 한 줄도 생략하거나 축약하지 마세요
5. 주석은 유지하되 필요시 변환 관련 주석 추가 가능
"""

    response = call_llm(prompt)
    
    if not response:
        print(f"     [ERROR] 변환 실패")
        return None
    
    # 코드 블록 제거
    oracle_code = response.strip()
    oracle_code = re.sub(r'^```sql\s*\n?', '', oracle_code)
    oracle_code = re.sub(r'^```\s*\n?', '', oracle_code)
    oracle_code = re.sub(r'\n?```$', '', oracle_code)
    oracle_code = oracle_code.strip()
    
    print(f"     [INFO] 변환 완료 ({len(oracle_code)} 문자)")
    
    # 로그 저장
    save_log(f"chunk_{chunk['chunk_id']:03d}_original.sql", sql_content)
    save_log(f"chunk_{chunk['chunk_id']:03d}_converted.sql", oracle_code)
    
    # API 호출 간 대기
    time.sleep(DELAY_BETWEEN_CALLS)
    
    return oracle_code

def convert_all_chunks(chunks, sql_lines):
    """모든 청크를 변환"""
    
    print("\n[STEP 2/3] 청크별 Oracle 변환 중...")
    
    converted_chunks = []
    total = len(chunks)
    
    for i, chunk in enumerate(chunks, 1):
        converted = convert_chunk(chunk, sql_lines, i, total)
        
        if converted:
            converted_chunks.append({
                'chunk_id': chunk['chunk_id'],
                'name': chunk['name'],
                'start_line': chunk['start_line'],
                'end_line': chunk['end_line'],
                'parent_context': chunk.get('parent_context', ''),
                'oracle_code': converted
            })
        else:
            print(f"  [ERROR] 청크 {chunk['chunk_id']} 변환 실패")
            original_sql = '\n'.join(sql_lines[chunk['start_line']-1:chunk['end_line']])
            converted_chunks.append({
                'chunk_id': chunk['chunk_id'],
                'name': chunk['name'],
                'start_line': chunk['start_line'],
                'end_line': chunk['end_line'],
                'parent_context': chunk.get('parent_context', ''),
                'oracle_code': f"-- [ERROR] 변환 실패\n-- {original_sql.replace(chr(10), chr(10) + '-- ')}"
            })
    
    success_count = len([c for c in converted_chunks if '[ERROR]' not in c['oracle_code']])
    print(f"\n[INFO] {success_count}/{total} 청크 변환 완료")
    
    return converted_chunks

# ============================================================================
# 청크 병합
# ============================================================================

def merge_chunks(converted_chunks, analysis):
    """변환된 청크들을 하나의 Oracle 프로시저로 병합"""
    
    print("\n[STEP 3/3] 변환된 청크 병합 중...")
    
    # 청크 ID 순으로 정렬
    converted_chunks.sort(key=lambda x: x['chunk_id'])
    
    merged_sql = []
    
    # 헤더
    merged_sql.append("-- ============================================================================")
    merged_sql.append("-- PostgreSQL to Oracle 자동 변환 결과")
    merged_sql.append(f"-- 원본 파일: {SQL_FILE_PATH}")
    merged_sql.append(f"-- 변환 일시: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    merged_sql.append(f"-- 총 청크 수: {len(converted_chunks)}")
    merged_sql.append("-- ============================================================================")
    merged_sql.append("")
    
    # 각 청크 병합
    for chunk in converted_chunks:
        merged_sql.append(f"-- [Chunk {chunk['chunk_id']}] {chunk['name']}")
        merged_sql.append(f"-- Lines: {chunk['start_line']}-{chunk['end_line']}")
        if chunk.get('parent_context'):
            merged_sql.append(f"-- Context: {chunk['parent_context']}")
        merged_sql.append("-- " + "-" * 76)
        merged_sql.append(chunk['oracle_code'])
        merged_sql.append("")
    
    result = '\n'.join(merged_sql)
    
    print(f"[INFO] 병합 완료 ({len(result)} 문자)")
    
    return result

# ============================================================================
# 로그 저장
# ============================================================================

def save_log(filename, content):
    """로그 파일 저장"""
    
    log_path = Path(LOG_DIR)
    log_path.mkdir(exist_ok=True)
    
    filepath = log_path / filename
    
    if isinstance(content, dict):
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(content, f, indent=2, ensure_ascii=False)
    else:
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(str(content))

# ============================================================================
# 메인 실행
# ============================================================================

def main():
    """메인 실행 함수"""
    
    print("=" * 80)
    print("PostgreSQL to Oracle 완전 자동 변환 시스템")
    print("=" * 80 + "\n")
    
    start_time = time.time()
    
    try:
        # 1. 파일 로드
        sql_lines, ast_data = load_files()
        
        # 2. AST 분석 (리프 노드까지 재귀)
        analysis = analyze_and_chunk_direct(sql_lines, ast_data)
        if not analysis:
            print("\n[ERROR] 분석 실패. 프로그램을 종료합니다.")
            return
        
        # 3. 청크별 변환
        converted_chunks = convert_all_chunks(analysis['chunks'], sql_lines)
        
        if not converted_chunks:
            print("\n[ERROR] 변환된 청크가 없습니다.")
            return
        
        # 4. 청크 병합
        final_sql = merge_chunks(converted_chunks, analysis)
        
        # 5. 최종 파일 저장
        with open(OUTPUT_SQL_FILE, 'w', encoding='utf-8') as f:
            f.write(final_sql)
        
        elapsed_time = time.time() - start_time
        
        print("\n" + "=" * 80)
        print("변환 완료")
        print("=" * 80)
        print(f"\n출력 파일: {OUTPUT_SQL_FILE}")
        print(f"통계:")
        print(f"  - 원본 라인 수: {len(sql_lines)}")
        print(f"  - 총 청크 수: {len(analysis['chunks'])}")
        print(f"  - 변환 성공: {len([c for c in converted_chunks if '[ERROR]' not in c['oracle_code']])}")
        print(f"  - 소요 시간: {elapsed_time:.1f}초")
        print(f"\n로그: {LOG_DIR}/")
        print(f"\n[WARNING] 변환된 SQL을 실행 전 반드시 검토하세요\n")
        
    except Exception as e:
        print(f"\n[ERROR] 오류: {e}")
        import traceback
        traceback.print_exc()
        save_log("error.txt", traceback.format_exc())

if __name__ == "__main__":
    main()
