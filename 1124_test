#!/usr/bin/env python3
"""
PostgreSQL to Oracle Procedure Converter
AST 기반 청크 분할 및 LLM 변환 스크립트
"""

import json
import os
import time
import asyncio
import aiohttp
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from pathlib import Path
import logging

# ============================================================================
# 설정
# ============================================================================
SQL_FILE_PATH = "./postgres_procedure.sql"
AST_JSON_PATH = "./procedure_ast.json"
OUTPUT_SQL_FILE = "./oracle_procedure.sql"
LOG_DIR = "./conversion_logs"

LLM_API_URL = "https://company-url.com/api/chat"
LLM_API_KEY = "api-key"
LLM_MODEL = "gpt-4.1"

# 설정 상수
MAX_RETRIES = 3
REQUEST_TIMEOUT = 180
REQUEST_DELAY = 2  # 초
MAX_CHUNK_LINES = 500
MAX_CONCURRENT_REQUESTS = 5  # 동시 요청 수

# ============================================================================
# 로깅 설정
# ============================================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(f'{LOG_DIR}/conversion.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ============================================================================
# 데이터 클래스
# ============================================================================
@dataclass
class Chunk:
    """변환할 청크 정보"""
    chunk_id: int
    node_type: str
    start_line: int
    end_line: int
    content: str
    parent_context: str
    is_leaf: bool
    
    def line_count(self) -> int:
        return self.end_line - self.start_line + 1


@dataclass
class TreeNode:
    """트리 구조 노드"""
    node_type: str
    start_line: int
    end_line: int
    parent_context: str
    prefix: Optional[str] = None
    suffix: Optional[str] = None
    children: Optional[List['TreeNode']] = None
    converted_prefix: Optional[str] = None
    converted_suffix: Optional[str] = None
    converted_content: Optional[str] = None  # 리프 노드용
    is_leaf: bool = False


# ============================================================================
# SQL 파일 로드
# ============================================================================
def load_sql_file(file_path: str) -> List[str]:
    """SQL 파일을 라인 배열로 로드"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        logger.info(f"SQL 파일 로드 완료: {len(lines)} 라인")
        return lines
    except Exception as e:
        logger.error(f"SQL 파일 로드 실패: {e}")
        raise


def load_ast_json(file_path: str) -> Dict[str, Any]:
    """AST JSON 파일 로드"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            ast = json.load(f)
        logger.info(f"AST JSON 로드 완료")
        return ast
    except Exception as e:
        logger.error(f"AST JSON 로드 실패: {e}")
        raise


# ============================================================================
# 청크 추출 (AST 재귀 순회)
# ============================================================================
def extract_lines(sql_lines: List[str], start_line: int, end_line: int) -> str:
    """지정된 라인 범위의 SQL 추출 (1-based index)"""
    if start_line > end_line or start_line < 1:
        return ""
    return ''.join(sql_lines[start_line-1:end_line])


def build_tree_structure(
    node: Dict[str, Any],
    sql_lines: List[str],
    parent_context: str = ""
) -> TreeNode:
    """
    AST를 재귀적으로 순회하여 트리 구조 구축
    절대 원칙: children이 있는 노드는 무조건 리프 노드까지 재귀 탐색
    """
    node_type = node.get('type', 'UNKNOWN')
    start_line = node.get('startLine', 1)
    end_line = node.get('endLine', 1)
    children = node.get('children', [])
    
    # 현재 컨텍스트 경로
    current_context = f"{parent_context} > {node_type}" if parent_context else node_type
    
    tree_node = TreeNode(
        node_type=node_type,
        start_line=start_line,
        end_line=end_line,
        parent_context=current_context
    )
    
    # children이 없으면 리프 노드
    if not children:
        tree_node.is_leaf = True
        tree_node.converted_content = extract_lines(sql_lines, start_line, end_line)
        return tree_node
    
    # children이 있으면 컨테이너 노드 - 반드시 재귀 탐색
    tree_node.is_leaf = False
    tree_node.children = []
    
    # PREFIX 추출: 부모 시작 ~ 첫 자식 시작-1
    first_child_start = children[0].get('startLine', start_line)
    if first_child_start > start_line:
        tree_node.prefix = extract_lines(sql_lines, start_line, first_child_start - 1)
    
    # 모든 자식 재귀 처리 (절대 원칙!)
    for child in children:
        child_tree = build_tree_structure(child, sql_lines, current_context)
        tree_node.children.append(child_tree)
    
    # SUFFIX 추출: 마지막 자식 끝+1 ~ 부모 끝
    last_child_end = children[-1].get('endLine', end_line)
    if last_child_end < end_line:
        tree_node.suffix = extract_lines(sql_lines, last_child_end + 1, end_line)
    
    return tree_node


def collect_leaf_chunks(tree_node: TreeNode, chunks: List[Chunk], chunk_id_counter: List[int]):
    """트리에서 모든 리프 노드(청크)를 수집"""
    if tree_node.is_leaf:
        # 리프 노드 = 청크
        chunk = Chunk(
            chunk_id=chunk_id_counter[0],
            node_type=tree_node.node_type,
            start_line=tree_node.start_line,
            end_line=tree_node.end_line,
            content=tree_node.converted_content,
            parent_context=tree_node.parent_context,
            is_leaf=True
        )
        chunks.append(chunk)
        chunk_id_counter[0] += 1
        
        # 500 라인 초과 경고
        if chunk.line_count() > MAX_CHUNK_LINES:
            logger.warning(
                f"청크 {chunk.chunk_id} 라인 수 초과: {chunk.line_count()} 라인 "
                f"({chunk.start_line}-{chunk.end_line})"
            )
    else:
        # PREFIX도 변환 대상
        if tree_node.prefix:
            chunk = Chunk(
                chunk_id=chunk_id_counter[0],
                node_type=f"{tree_node.node_type}_PREFIX",
                start_line=tree_node.start_line,
                end_line=tree_node.children[0].start_line - 1 if tree_node.children else tree_node.start_line,
                content=tree_node.prefix,
                parent_context=tree_node.parent_context,
                is_leaf=False
            )
            chunks.append(chunk)
            chunk_id_counter[0] += 1
        
        # 자식 재귀
        if tree_node.children:
            for child in tree_node.children:
                collect_leaf_chunks(child, chunks, chunk_id_counter)
        
        # SUFFIX도 변환 대상
        if tree_node.suffix:
            chunk = Chunk(
                chunk_id=chunk_id_counter[0],
                node_type=f"{tree_node.node_type}_SUFFIX",
                start_line=tree_node.children[-1].end_line + 1 if tree_node.children else tree_node.end_line,
                end_line=tree_node.end_line,
                content=tree_node.suffix,
                parent_context=tree_node.parent_context,
                is_leaf=False
            )
            chunks.append(chunk)
            chunk_id_counter[0] += 1


# ============================================================================
# LLM 변환
# ============================================================================
def create_conversion_prompt(chunk: Chunk) -> str:
    """변환 프롬프트 생성"""
    return f"""당신은 PostgreSQL을 Oracle 19c로 변환하는 전문가입니다.

다음 PostgreSQL 코드를 Oracle 19c로 변환하세요.

**컨텍스트 경로:** {chunk.parent_context}
**노드 타입:** {chunk.node_type}
**라인 범위:** {chunk.start_line}-{chunk.end_line}

**변환 규칙:**

함수 변환:
- COALESCE() → NVL()
- date_part(field, date) → EXTRACT(field FROM date)
- substring() → SUBSTR()
- now() → SYSDATE
- string_agg() → LISTAGG()

타입 변환:
- VARCHAR → VARCHAR2
- INTEGER → NUMBER
- BOOLEAN → NUMBER(1)
- refcursor → SYS_REFCURSOR

구조 변환:
- CREATE FUNCTION → CREATE PROCEDURE (RETURNS void인 경우)
- $$ → AS, END $$ → END; /
- PERFORM → SELECT INTO v_dummy FROM dual
- RAISE NOTICE → DBMS_OUTPUT.PUT_LINE
- CREATE TEMP TABLE → TRUNCATE TABLE

**중요 제약사항:**
1. 축약, 생략, 요약 절대 금지 - 모든 로직을 빠짐없이 변환
2. SQL은 한 글자만 달라져도 오류 발생 - 정확하게 변환
3. 코드 블록 마커(```)를 사용하지 말고 순수 SQL만 출력
4. 주석이나 설명 없이 변환된 SQL 코드만 출력

**PostgreSQL 코드:**
```sql
{chunk.content}
```

**변환된 Oracle 코드를 출력하세요:**"""


async def convert_chunk_with_llm(
    session: aiohttp.ClientSession,
    chunk: Chunk,
    semaphore: asyncio.Semaphore
) -> tuple[Chunk, str]:
    """LLM API를 호출하여 청크 변환 (비동기)"""
    async with semaphore:
        system_message = "당신은 PostgreSQL을 Oracle 19c로 변환하는 전문가입니다. 정확하고 완전한 변환을 수행하세요."
        user_message = create_conversion_prompt(chunk)
        
        payload = {
            "messages": [
                {"role": "system", "content": system_message},
                {"role": "user", "content": user_message}
            ],
            "model": LLM_MODEL
        }
        
        headers = {
            "Authorization": f"Bearer {LLM_API_KEY}",
            "Content-Type": "application/json"
        }
        
        for attempt in range(MAX_RETRIES):
            try:
                logger.info(f"청크 {chunk.chunk_id} 변환 시도 {attempt + 1}/{MAX_RETRIES}")
                
                async with session.post(
                    LLM_API_URL,
                    json=payload,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=REQUEST_TIMEOUT)
                ) as response:
                    response.raise_for_status()
                    result = await response.json()
                    
                    # 응답에서 텍스트 추출 (API 형식에 따라 조정 필요)
                    converted = result.get('text', '') or result.get('content', '') or result.get('message', '')
                    
                    # 코드 블록 마커 제거
                    converted = converted.replace('```sql', '').replace('```', '').strip()
                    
                    logger.info(f"청크 {chunk.chunk_id} 변환 완료")
                    
                    # 딜레이
                    await asyncio.sleep(REQUEST_DELAY)
                    
                    return chunk, converted
                    
            except asyncio.TimeoutError:
                logger.warning(f"청크 {chunk.chunk_id} 타임아웃 (시도 {attempt + 1})")
                if attempt == MAX_RETRIES - 1:
                    raise
                await asyncio.sleep(REQUEST_DELAY * 2)
                
            except Exception as e:
                logger.error(f"청크 {chunk.chunk_id} 변환 실패 (시도 {attempt + 1}): {e}")
                if attempt == MAX_RETRIES - 1:
                    raise
                await asyncio.sleep(REQUEST_DELAY * 2)
        
        raise Exception(f"청크 {chunk.chunk_id} 변환 최종 실패")


async def convert_all_chunks(chunks: List[Chunk]) -> Dict[int, str]:
    """모든 청크를 병렬로 변환"""
    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)
    converted_chunks = {}
    
    async with aiohttp.ClientSession() as session:
        tasks = [convert_chunk_with_llm(session, chunk, semaphore) for chunk in chunks]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"변환 실패: {result}")
                continue
            
            chunk, converted = result
            converted_chunks[chunk.chunk_id] = converted
            
            # 로그 저장
            save_chunk_log(chunk, converted)
    
    return converted_chunks


def save_chunk_log(chunk: Chunk, converted: str):
    """청크별 로그 저장"""
    os.makedirs(LOG_DIR, exist_ok=True)
    
    # 원본 저장
    original_path = f"{LOG_DIR}/chunk_{chunk.chunk_id:03d}_original.sql"
    with open(original_path, 'w', encoding='utf-8') as f:
        f.write(f"-- Chunk {chunk.chunk_id}\n")
        f.write(f"-- Type: {chunk.node_type}\n")
        f.write(f"-- Context: {chunk.parent_context}\n")
        f.write(f"-- Lines: {chunk.start_line}-{chunk.end_line}\n")
        f.write(f"-- Line Count: {chunk.line_count()}\n\n")
        f.write(chunk.content)
    
    # 변환 저장
    converted_path = f"{LOG_DIR}/chunk_{chunk.chunk_id:03d}_converted.sql"
    with open(converted_path, 'w', encoding='utf-8') as f:
        f.write(f"-- Chunk {chunk.chunk_id} (Converted)\n")
        f.write(f"-- Type: {chunk.node_type}\n")
        f.write(f"-- Context: {chunk.parent_context}\n")
        f.write(f"-- Lines: {chunk.start_line}-{chunk.end_line}\n\n")
        f.write(converted)


# ============================================================================
# 트리 재구성 및 렌더링
# ============================================================================
def apply_conversions_to_tree(
    tree_node: TreeNode,
    converted_chunks: Dict[int, str],
    chunk_map: Dict[tuple, int]
):
    """변환 결과를 트리에 적용"""
    if tree_node.is_leaf:
        # 리프 노드 변환 적용
        key = (tree_node.start_line, tree_node.end_line, True)
        chunk_id = chunk_map.get(key)
        if chunk_id is not None:
            tree_node.converted_content = converted_chunks.get(chunk_id, tree_node.converted_content)
    else:
        # PREFIX 변환 적용
        if tree_node.prefix:
            first_child_start = tree_node.children[0].start_line if tree_node.children else tree_node.start_line
            key = (tree_node.start_line, first_child_start - 1, False)
            chunk_id = chunk_map.get(key)
            if chunk_id is not None:
                tree_node.converted_prefix = converted_chunks.get(chunk_id, tree_node.prefix)
        
        # 자식 재귀
        if tree_node.children:
            for child in tree_node.children:
                apply_conversions_to_tree(child, converted_chunks, chunk_map)
        
        # SUFFIX 변환 적용
        if tree_node.suffix:
            last_child_end = tree_node.children[-1].end_line if tree_node.children else tree_node.end_line
            key = (last_child_end + 1, tree_node.end_line, False)
            chunk_id = chunk_map.get(key)
            if chunk_id is not None:
                tree_node.converted_suffix = converted_chunks.get(chunk_id, tree_node.suffix)


def render_tree(tree_node: TreeNode) -> str:
    """트리를 재귀적으로 렌더링하여 최종 SQL 생성"""
    if tree_node.is_leaf:
        return tree_node.converted_content or ""
    
    result = []
    
    # PREFIX
    if tree_node.converted_prefix:
        result.append(tree_node.converted_prefix)
    elif tree_node.prefix:
        result.append(tree_node.prefix)
    
    # 자식들
    if tree_node.children:
        for child in tree_node.children:
            result.append(render_tree(child))
    
    # SUFFIX
    if tree_node.converted_suffix:
        result.append(tree_node.converted_suffix)
    elif tree_node.suffix:
        result.append(tree_node.suffix)
    
    return ''.join(result)


# ============================================================================
# 메인 실행
# ============================================================================
def main():
    """메인 실행 함수"""
    start_time = time.time()
    
    # 로그 디렉토리 생성
    os.makedirs(LOG_DIR, exist_ok=True)
    
    logger.info("=" * 80)
    logger.info("PostgreSQL to Oracle 프로시저 자동 변환 시작")
    logger.info("=" * 80)
    
    # 1. 파일 로드
    logger.info("1단계: 파일 로드")
    sql_lines = load_sql_file(SQL_FILE_PATH)
    ast_data = load_ast_json(AST_JSON_PATH)
    
    # 2. 트리 구조 구축
    logger.info("2단계: AST 트리 구조 구축")
    tree_root = build_tree_structure(ast_data, sql_lines)
    
    # 3. 청크 수집
    logger.info("3단계: 리프 노드 청크 수집")
    chunks = []
    chunk_id_counter = [1]
    collect_leaf_chunks(tree_root, chunks, chunk_id_counter)
    logger.info(f"총 {len(chunks)}개 청크 수집 완료")
    
    # 청크 맵 생성 (나중에 트리에 적용하기 위함)
    chunk_map = {}
    for chunk in chunks:
        key = (chunk.start_line, chunk.end_line, chunk.is_leaf)
        chunk_map[key] = chunk.chunk_id
    
    # 4. 청크별 변환 (병렬 처리)
    logger.info("4단계: LLM 청크 변환 (병렬 처리)")
    converted_chunks = asyncio.run(convert_all_chunks(chunks))
    logger.info(f"변환 완료: {len(converted_chunks)}/{len(chunks)} 청크")
    
    # 5. 트리에 변환 결과 적용
    logger.info("5단계: 트리에 변환 결과 적용")
    apply_conversions_to_tree(tree_root, converted_chunks, chunk_map)
    
    # 6. 최종 SQL 렌더링
    logger.info("6단계: 최종 Oracle SQL 렌더링")
    final_sql = render_tree(tree_root)
    
    # 7. 파일 저장
    logger.info("7단계: 최종 파일 저장")
    with open(OUTPUT_SQL_FILE, 'w', encoding='utf-8') as f:
        f.write(final_sql)
    
    # 분석 결과 저장
    analysis = {
        "total_chunks": len(chunks),
        "converted_chunks": len(converted_chunks),
        "failed_chunks": len(chunks) - len(converted_chunks),
        "total_lines": len(sql_lines),
        "chunks_detail": [
            {
                "chunk_id": chunk.chunk_id,
                "type": chunk.node_type,
                "context": chunk.parent_context,
                "lines": f"{chunk.start_line}-{chunk.end_line}",
                "line_count": chunk.line_count(),
                "converted": chunk.chunk_id in converted_chunks
            }
            for chunk in chunks
        ]
    }
    
    with open(f"{LOG_DIR}/analysis_result.json", 'w', encoding='utf-8') as f:
        json.dump(analysis, f, indent=2, ensure_ascii=False)
    
    elapsed_time = time.time() - start_time
    
    logger.info("=" * 80)
    logger.info("변환 완료!")
    logger.info(f"실행 시간: {elapsed_time:.2f}초")
    logger.info(f"출력 파일: {OUTPUT_SQL_FILE}")
    logger.info(f"로그 디렉토리: {LOG_DIR}")
    logger.info("=" * 80)


if __name__ == "__main__":
    main()
