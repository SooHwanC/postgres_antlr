#!/usr/bin/env python3
"""
PostgreSQL to Oracle Procedure Converter
AST 기반 청크 분할 및 LLM 변환 스크립트
"""

import json
import os
import time
import asyncio
import aiohttp
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from pathlib import Path
import logging

# ============================================================================
# 설정
# ============================================================================
SQL_FILE_PATH = "./postgres_procedure.sql"
AST_JSON_PATH = "./procedure_ast.json"
OUTPUT_SQL_FILE = "./oracle_procedure.sql"
LOG_DIR = "./conversion_logs"

LLM_API_URL = "https://company-url.com/api/chat"
LLM_API_KEY = "api-key"
LLM_MODEL = "gpt-4.1"

# 설정 상수
MAX_RETRIES = 3
REQUEST_TIMEOUT = 180
REQUEST_DELAY = 2  # 초
MAX_CHUNK_LINES = 500
MAX_CONCURRENT_REQUESTS = 5  # 동시 요청 수

# ============================================================================
# 로깅 설정
# ============================================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(f'{LOG_DIR}/conversion.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ============================================================================
# 데이터 클래스
# ============================================================================
@dataclass
class Chunk:
    """변환할 청크 정보"""
    chunk_id: int
    node_type: str
    start_line: int
    end_line: int
    content: str
    parent_context: str
    is_leaf: bool
    sequence: int  # 청크의 순서 (병합 시 사용)
    
    def line_count(self) -> int:
        return self.end_line - self.start_line + 1


@dataclass
class TreeNode:
    """트리 구조 노드 (순회용)"""
    node_type: str
    start_line: int
    end_line: int
    parent_context: str
    children: Optional[List['TreeNode']] = None
    is_leaf: bool = False


# ============================================================================
# SQL 파일 로드
# ============================================================================
def load_sql_file(file_path: str) -> List[str]:
    """SQL 파일을 라인 배열로 로드"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        logger.info(f"SQL 파일 로드 완료: {len(lines)} 라인")
        return lines
    except Exception as e:
        logger.error(f"SQL 파일 로드 실패: {e}")
        raise


def load_ast_json(file_path: str) -> Dict[str, Any]:
    """AST JSON 파일 로드"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            ast = json.load(f)
        logger.info(f"AST JSON 로드 완료")
        return ast
    except Exception as e:
        logger.error(f"AST JSON 로드 실패: {e}")
        raise


# ============================================================================
# 청크 추출 (AST 재귀 순회)
# ============================================================================
def extract_lines(sql_lines: List[str], start_line: int, end_line: int) -> str:
    """지정된 라인 범위의 SQL 추출 (1-based index)"""
    if start_line > end_line or start_line < 1:
        return ""
    return ''.join(sql_lines[start_line-1:end_line])


def build_tree_structure(
    node: Dict[str, Any],
    sql_lines: List[str],
    parent_context: str = ""
) -> TreeNode:
    """
    AST를 재귀적으로 순회하여 트리 구조 구축
    절대 원칙: children이 있는 노드는 무조건 리프 노드까지 재귀 탐색
    """
    node_type = node.get('type', 'UNKNOWN')
    start_line = node.get('startLine', 1)
    end_line = node.get('endLine', 1)
    children = node.get('children', [])
    
    # 현재 컨텍스트 경로
    current_context = f"{parent_context} > {node_type}" if parent_context else node_type
    
    tree_node = TreeNode(
        node_type=node_type,
        start_line=start_line,
        end_line=end_line,
        parent_context=current_context
    )
    
    # children이 없으면 리프 노드
    if not children:
        tree_node.is_leaf = True
        return tree_node
    
    # children이 있으면 컨테이너 노드 - 반드시 재귀 탐색
    tree_node.is_leaf = False
    tree_node.children = []
    
    # 모든 자식 재귀 처리 (절대 원칙!)
    for child in children:
        child_tree = build_tree_structure(child, sql_lines, current_context)
        tree_node.children.append(child_tree)
    
    return tree_node


def collect_chunks_in_order(
    tree_node: TreeNode, 
    sql_lines: List[str],
    chunks: List[Chunk], 
    chunk_id_counter: List[int],
    sequence_counter: List[int]
):
    """
    트리를 순회하면서 청크를 순서대로 수집
    PREFIX -> 자식들 -> SUFFIX 순서로 수집하여 나중에 순서대로 병합 가능
    """
    if tree_node.is_leaf:
        # 리프 노드 = 하나의 청크
        content = extract_lines(sql_lines, tree_node.start_line, tree_node.end_line)
        chunk = Chunk(
            chunk_id=chunk_id_counter[0],
            node_type=tree_node.node_type,
            start_line=tree_node.start_line,
            end_line=tree_node.end_line,
            content=content,
            parent_context=tree_node.parent_context,
            is_leaf=True,
            sequence=sequence_counter[0]
        )
        chunks.append(chunk)
        chunk_id_counter[0] += 1
        sequence_counter[0] += 1
        
        # 500 라인 초과 경고
        if chunk.line_count() > MAX_CHUNK_LINES:
            logger.warning(
                f"청크 {chunk.chunk_id} 라인 수 초과: {chunk.line_count()} 라인 "
                f"({chunk.start_line}-{chunk.end_line})"
            )
    else:
        # 컨테이너 노드 - PREFIX, 자식들, SUFFIX 순서로 처리
        
        # 1. PREFIX 추출 및 청크 생성
        if tree_node.children:
            first_child_start = tree_node.children[0].start_line
            if first_child_start > tree_node.start_line:
                prefix_content = extract_lines(sql_lines, tree_node.start_line, first_child_start - 1)
                chunk = Chunk(
                    chunk_id=chunk_id_counter[0],
                    node_type=f"{tree_node.node_type}_PREFIX",
                    start_line=tree_node.start_line,
                    end_line=first_child_start - 1,
                    content=prefix_content,
                    parent_context=tree_node.parent_context,
                    is_leaf=False,
                    sequence=sequence_counter[0]
                )
                chunks.append(chunk)
                chunk_id_counter[0] += 1
                sequence_counter[0] += 1
        
        # 2. 자식 재귀 처리 (순서대로)
        if tree_node.children:
            for child in tree_node.children:
                collect_chunks_in_order(child, sql_lines, chunks, chunk_id_counter, sequence_counter)
        
        # 3. SUFFIX 추출 및 청크 생성
        if tree_node.children:
            last_child_end = tree_node.children[-1].end_line
            if last_child_end < tree_node.end_line:
                suffix_content = extract_lines(sql_lines, last_child_end + 1, tree_node.end_line)
                chunk = Chunk(
                    chunk_id=chunk_id_counter[0],
                    node_type=f"{tree_node.node_type}_SUFFIX",
                    start_line=last_child_end + 1,
                    end_line=tree_node.end_line,
                    content=suffix_content,
                    parent_context=tree_node.parent_context,
                    is_leaf=False,
                    sequence=sequence_counter[0]
                )
                chunks.append(chunk)
                chunk_id_counter[0] += 1
                sequence_counter[0] += 1


# ============================================================================
# LLM 변환
# ============================================================================
def create_conversion_prompt(chunk: Chunk) -> str:
    """변환 프롬프트 생성"""
    return f"""당신은 PostgreSQL을 Oracle 19c로 변환하는 전문가입니다.

다음 PostgreSQL 코드를 Oracle 19c로 변환하세요.

**컨텍스트 경로:** {chunk.parent_context}
**노드 타입:** {chunk.node_type}
**라인 범위:** {chunk.start_line}-{chunk.end_line}

**변환 규칙:**

함수 변환:
- COALESCE() → NVL()
- date_part(field, date) → EXTRACT(field FROM date)
- substring() → SUBSTR()
- now() → SYSDATE
- string_agg() → LISTAGG()

타입 변환:
- VARCHAR → VARCHAR2
- INTEGER → NUMBER
- BOOLEAN → NUMBER(1)
- refcursor → SYS_REFCURSOR

구조 변환:
- CREATE FUNCTION → CREATE PROCEDURE (RETURNS void인 경우)
- $$ → AS, END $$ → END; /
- PERFORM → SELECT INTO v_dummy FROM dual
- RAISE NOTICE → DBMS_OUTPUT.PUT_LINE
- CREATE TEMP TABLE → TRUNCATE TABLE

**중요 제약사항:**
1. 축약, 생략, 요약 절대 금지 - 모든 로직을 빠짐없이 변환
2. SQL은 한 글자만 달라져도 오류 발생 - 정확하게 변환
3. 코드 블록 마커(```)를 사용하지 말고 순수 SQL만 출력
4. 주석이나 설명 없이 변환된 SQL 코드만 출력

**PostgreSQL 코드:**
```sql
{chunk.content}
```

**변환된 Oracle 코드를 출력하세요:**"""


async def convert_chunk_with_llm(
    session: aiohttp.ClientSession,
    chunk: Chunk,
    semaphore: asyncio.Semaphore
) -> tuple[Chunk, str]:
    """LLM API를 호출하여 청크 변환 (비동기)"""
    async with semaphore:
        system_message = "당신은 PostgreSQL을 Oracle 19c로 변환하는 전문가입니다. 정확하고 완전한 변환을 수행하세요."
        user_message = create_conversion_prompt(chunk)
        
        payload = {
            "messages": [
                {"role": "system", "content": system_message},
                {"role": "user", "content": user_message}
            ],
            "model": LLM_MODEL
        }
        
        headers = {
            "Authorization": f"Bearer {LLM_API_KEY}",
            "Content-Type": "application/json"
        }
        
        for attempt in range(MAX_RETRIES):
            try:
                logger.info(f"청크 {chunk.chunk_id} 변환 시도 {attempt + 1}/{MAX_RETRIES}")
                
                async with session.post(
                    LLM_API_URL,
                    json=payload,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=REQUEST_TIMEOUT)
                ) as response:
                    response.raise_for_status()
                    
                    # 텍스트 응답 직접 읽기
                    converted = await response.text()
                    
                    # 코드 블록 마커 제거
                    converted = converted.replace('```sql', '').replace('```', '').strip()
                    
                    logger.info(f"청크 {chunk.chunk_id} 변환 완료")
                    
                    # 딜레이
                    await asyncio.sleep(REQUEST_DELAY)
                    
                    return chunk, converted
                    
            except asyncio.TimeoutError:
                logger.warning(f"청크 {chunk.chunk_id} 타임아웃 (시도 {attempt + 1})")
                if attempt == MAX_RETRIES - 1:
                    raise
                await asyncio.sleep(REQUEST_DELAY * 2)
                
            except Exception as e:
                logger.error(f"청크 {chunk.chunk_id} 변환 실패 (시도 {attempt + 1}): {e}")
                if attempt == MAX_RETRIES - 1:
                    raise
                await asyncio.sleep(REQUEST_DELAY * 2)
        
        raise Exception(f"청크 {chunk.chunk_id} 변환 최종 실패")


async def convert_all_chunks(chunks: List[Chunk]) -> Dict[int, str]:
    """모든 청크를 병렬로 변환"""
    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)
    converted_chunks = {}
    
    async with aiohttp.ClientSession() as session:
        tasks = [convert_chunk_with_llm(session, chunk, semaphore) for chunk in chunks]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"변환 실패: {result}")
                continue
            
            chunk, converted = result
            converted_chunks[chunk.chunk_id] = converted
            
            # 로그 저장
            save_chunk_log(chunk, converted)
    
    return converted_chunks


def save_chunk_log(chunk: Chunk, converted: str):
    """청크별 로그 저장"""
    os.makedirs(LOG_DIR, exist_ok=True)
    
    # 원본 저장
    original_path = f"{LOG_DIR}/chunk_{chunk.chunk_id:03d}_original.sql"
    with open(original_path, 'w', encoding='utf-8') as f:
        f.write(f"-- Chunk {chunk.chunk_id}\n")
        f.write(f"-- Type: {chunk.node_type}\n")
        f.write(f"-- Context: {chunk.parent_context}\n")
        f.write(f"-- Lines: {chunk.start_line}-{chunk.end_line}\n")
        f.write(f"-- Line Count: {chunk.line_count()}\n\n")
        f.write(chunk.content)
    
    # 변환 저장
    converted_path = f"{LOG_DIR}/chunk_{chunk.chunk_id:03d}_converted.sql"
    with open(converted_path, 'w', encoding='utf-8') as f:
        f.write(f"-- Chunk {chunk.chunk_id} (Converted)\n")
        f.write(f"-- Type: {chunk.node_type}\n")
        f.write(f"-- Context: {chunk.parent_context}\n")
        f.write(f"-- Lines: {chunk.start_line}-{chunk.end_line}\n\n")
        f.write(converted)


# ============================================================================
# 트리 재구성 및 렌더링
# ============================================================================
def merge_converted_chunks(chunks: List[Chunk], converted_chunks: Dict[int, str]) -> str:
    """
    변환된 청크들을 순서대로 병합
    sequence 순서대로 정렬 후 그대로 이어붙임
    """
    # sequence 순서로 정렬
    sorted_chunks = sorted(chunks, key=lambda c: c.sequence)
    
    result = []
    for chunk in sorted_chunks:
        converted = converted_chunks.get(chunk.chunk_id)
        if converted:
            result.append(converted)
            if not converted.endswith('\n'):
                result.append('\n')
        else:
            # 변환 실패한 경우 원본을 주석 처리하여 포함
            logger.warning(f"청크 {chunk.chunk_id} 변환 실패 - 원본을 주석으로 포함")
            result.append(f"-- [변환 실패 - 원본]\n")
            result.append(f"-- {chunk.content.replace(chr(10), chr(10) + '-- ')}\n")
    
    return ''.join(result)


# ============================================================================
# 메인 실행
# ============================================================================
def main():
    """메인 실행 함수"""
    start_time = time.time()
    
    # 로그 디렉토리 생성
    os.makedirs(LOG_DIR, exist_ok=True)
    
    logger.info("=" * 80)
    logger.info("PostgreSQL to Oracle 프로시저 자동 변환 시작")
    logger.info("=" * 80)
    
    # 1. 파일 로드
    logger.info("1단계: 파일 로드")
    sql_lines = load_sql_file(SQL_FILE_PATH)
    ast_data = load_ast_json(AST_JSON_PATH)
    
    # 2. 트리 구조 구축
    logger.info("2단계: AST 트리 구조 구축")
    tree_root = build_tree_structure(ast_data, sql_lines)
    
    # 3. 청크 수집 (순서대로!)
    logger.info("3단계: 청크 순서대로 수집")
    chunks = []
    chunk_id_counter = [1]
    sequence_counter = [0]
    collect_chunks_in_order(tree_root, sql_lines, chunks, chunk_id_counter, sequence_counter)
    logger.info(f"총 {len(chunks)}개 청크 수집 완료 (순서: 0-{sequence_counter[0]-1})")
    
    # 4. 청크별 변환 (병렬 처리)
    logger.info("4단계: LLM 청크 변환 (병렬 처리)")
    converted_chunks = asyncio.run(convert_all_chunks(chunks))
    logger.info(f"변환 완료: {len(converted_chunks)}/{len(chunks)} 청크")
    
    # 5. 청크 병합 (순서대로!)
    logger.info("5단계: 변환된 청크를 순서대로 병합")
    final_sql = merge_converted_chunks(chunks, converted_chunks)
    
    # 6. 파일 저장
    logger.info("6단계: 최종 파일 저장")
    with open(OUTPUT_SQL_FILE, 'w', encoding='utf-8') as f:
        f.write(final_sql)
    
    # 분석 결과 저장
    analysis = {
        "total_chunks": len(chunks),
        "converted_chunks": len(converted_chunks),
        "failed_chunks": len(chunks) - len(converted_chunks),
        "total_lines": len(sql_lines),
        "chunks_detail": [
            {
                "chunk_id": chunk.chunk_id,
                "sequence": chunk.sequence,
                "type": chunk.node_type,
                "context": chunk.parent_context,
                "lines": f"{chunk.start_line}-{chunk.end_line}",
                "line_count": chunk.line_count(),
                "converted": chunk.chunk_id in converted_chunks
            }
            for chunk in chunks
        ]
    }
    
    with open(f"{LOG_DIR}/analysis_result.json", 'w', encoding='utf-8') as f:
        json.dump(analysis, f, indent=2, ensure_ascii=False)
    
    elapsed_time = time.time() - start_time
    
    logger.info("=" * 80)
    logger.info("변환 완료!")
    logger.info(f"실행 시간: {elapsed_time:.2f}초")
    logger.info(f"출력 파일: {OUTPUT_SQL_FILE}")
    logger.info(f"로그 디렉토리: {LOG_DIR}")
    logger.info("=" * 80)


if __name__ == "__main__":
    main()
